---
layout: post
title: "HDFS"
---

### Background
* store very large data sets reliably
* streaming at high bandwidth, not random file access
* large cluster
* Directly attached storage

### [HDFS Architecture] (http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html)

#### NameNode
* format NameNode
    * to create a file system instance, which is persistently stored on all nodes of the cluster
    * different cluster has different ID
* write data
    * NameNode nominate three DataNodes
    * client writes them in a pipeline fashion
* restart
    1. NameNode initializes the namespace image from the checkpoint
    - replay the journal log, which locates in local host native file system

#### DataNode
* start
    1. handshake to verify *namespace ID* and software version
    - register a *storage ID* for identifying DataNode
* live heartbeat
    * sent *block report* to NameNode every hour
    * heartbeat every 3s, 10 minutes are considered out of service
    * NameNode instructions are on replies of heartbeat

#### Image and Journal
* The namespace image is the file system metadata that describes the organization of application data as directories and files
* A persistent record of the image written to disk is called a *checkpoint*
    * it would be stored in multiple directories, e.g., in different volumes
    * once namespace information lost, it would be lost partly or entirely
    * if error on writing journal to one of the storage directories, it excludes that directory from the list of storage directories
    * NameNode shuts down if no storage directory is available
* Batch commits for multithreaded system
    * multiple transactions would be commited together to get rid of flush-and-sync operation

#### Checkpoint Node
* usually another node would read the checkpoint with journal to get a new checkpoint

#### Upgrade and Snapshot
* Snapshot creation is an all-cluster effort rather than a node-selective event
    * NameNode would create a new checkpoint
    * DataNode would copy-on-write: create a copy of storage directories with hard links
* On upgrade failure
    1. the namespace would roll back
    - NameNode would not recognize new files, new files on DataNodes would be deleted

#### Block Placement
* No DataNode contains > 1 replica of any block
* No rack contains > 2 replicas of the same block, provided there are sufficient racks

### Problems
* No authentications, file permissions -> use Kerberos
* Directory sub-tree should have quota

### Comments
#### Agree
* Snapshot would largely decrease the risk of upgrade failure. Since the upgrade frequency is low, it is acceptable to perform a overall snapshot
* The performance of this batch system is guaranteed by combine multiple transcations commits together
* Single Write simplifies file operations, and in the user case scenario, multiple writes rarely happens

#### Questionable
* Pipeline writes is not fast enough if datanodes is much larger than 3, in this case, the client would wait for a while to get packet ACK
    * I personally would config the `dfs.replication.min` as 2, and it would return as soon as data is written into 2 datanodes
* A whole cluster is limited solely by the master memory size, so it would be very inefficient to have large quantity of small files
* Secondary NameNode is always fall behind master, so there is a possibility that it would lose data when master fails permanently

### Reference
* [HDFS paper] (http://www.ituring.com.cn/article/4299)
* [HDFS写文件过程分析] (http://shiyanjun.cn/archives/942.html)
* [HDFS中的基本概念] (http://www.cnblogs.com/beanmoon/archive/2012/12/11/2809315.html)
